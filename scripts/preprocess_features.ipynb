{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(path):\n",
    "    \"\"\"Load and validate feature data with Persian-to-English conversion\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Persian digit mapping\n",
    "    persian_map = {\n",
    "        \"۰\": \"0\",\n",
    "        \"۱\": \"1\",\n",
    "        \"۲\": \"2\",\n",
    "        \"۳\": \"3\",\n",
    "        \"۴\": \"4\",\n",
    "        \"۵\": \"5\",\n",
    "        \"۶\": \"6\",\n",
    "        \"۷\": \"7\",\n",
    "        \"۸\": \"8\",\n",
    "        \"۹\": \"9\",\n",
    "    }\n",
    "\n",
    "    # Clean numeric columns\n",
    "    num_cols = [\"r\", \"rw\", \"rx\", \"rwc\", \"rwx\", \"rwxc\"]\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].astype(str).replace(persian_map, regex=True)\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Create binary labels\n",
    "    df[\"MALICIOUS\"] = df[\"label\"].map({\"B\": 0, \"M\": 1}).astype(int)\n",
    "\n",
    "    return df.drop(columns=[\"ID\", \"label\", \"sample\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c76519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"Feature calculations\"\"\"\n",
    "    # Replace zeros to avoid log(0)\n",
    "    df[\"rwxc_safe\"] = df[\"rwxc\"].replace(0, 1e-6)\n",
    "\n",
    "    # Revised features\n",
    "    df[\"perm_complexity\"] = 0.4 * df[\"rwx\"] + 0.3 * df[\"rwxc_safe\"] + 0.3 * df[\"rwc\"]\n",
    "    df[\"risk_score\"] = (\n",
    "        np.log1p(df[\"rwxc_safe\"]) * np.sqrt(df[\"rwx\"] + 1e-6)  # Avoid sqrt(0)\n",
    "    )\n",
    "\n",
    "    return df.drop(columns=[\"rwxc_safe\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(df):\n",
    "    \"\"\"Preprocessing Workflow with label handling\"\"\"\n",
    "    # Get feature names before dropping target\n",
    "    feature_names = df.drop(\"MALICIOUS\", axis=1).columns.tolist()\n",
    "\n",
    "    # Split FIRST to prevent data leakage\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df.drop(\"MALICIOUS\", axis=1),\n",
    "        df[\"MALICIOUS\"],\n",
    "        test_size=0.2,\n",
    "        stratify=df[\"MALICIOUS\"],  # Stratify on original labels\n",
    "        random_state=random.randint(10, 100),\n",
    "    )\n",
    "\n",
    "    # Print class distribution before SMOTE\n",
    "    print(\"Class counts before SMOTE:\", np.bincount(y_train))\n",
    "\n",
    "    # Apply SMOTE ONLY to training data\n",
    "    smote = SMOTE(sampling_strategy=\"auto\")\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "    print(\"Class counts after SMOTE:\", np.bincount(y_train_res))\n",
    "\n",
    "    # Normalize AFTER resampling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_final = scaler.fit_transform(X_train_res)\n",
    "    X_test_final = scaler.transform(X_test)\n",
    "\n",
    "    # Convert labels to numpy arrays to ensure consistent types\n",
    "    y_train_res = np.array(y_train_res)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    # Verify shapes and types\n",
    "    print(\"\\nShape verification:\")\n",
    "    print(f\"X_train: {X_train_final.shape}\")\n",
    "    print(f\"X_test: {X_test_final.shape}\")\n",
    "    print(f\"y_train: {y_train_res.shape}\")\n",
    "    print(f\"y_test: {y_test.shape}\")\n",
    "\n",
    "    return X_train_final, X_test_final, y_train_res, y_test, scaler, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b9a4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_data(X_train, X_test, y_train, y_test, feature_names):\n",
    "    \"\"\"Print a sample of preprocessed data for inspection\"\"\"\n",
    "    # Convert arrays back to DataFrames for readability\n",
    "    train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "    train_df[\"MALICIOUS\"] = y_train\n",
    "\n",
    "    test_df = pd.DataFrame(X_test, columns=feature_names)\n",
    "    test_df[\"MALICIOUS\"] = y_test\n",
    "\n",
    "    print(\"\\n=== Preprocessed Data Sample ===\")\n",
    "    print(\"\\nTraining Data (First 5 Rows):\")\n",
    "    print(train_df.head())\n",
    "\n",
    "    print(\"\\nTest Data (First 5 Rows):\")\n",
    "    print(test_df.head())\n",
    "\n",
    "    print(\"\\nData Summary:\")\n",
    "    print(test_df.describe())\n",
    "\n",
    "    print(\"\\nTraining Data Class Distribution:\")\n",
    "    print(pd.Series(y_train).value_counts())\n",
    "\n",
    "    print(\"\\nTest Data Class Distribution:\")\n",
    "    print(pd.Series(y_test).value_counts())\n",
    "\n",
    "    print(\"\\nFeature Statistics (Training):\")\n",
    "    print(train_df.drop(\"MALICIOUS\", axis=1).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96378002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_artifacts(X_train, X_test, y_train, y_test, scaler, feature_names):\n",
    "    \"\"\"Persist processed data\"\"\"\n",
    "    artifacts = {\n",
    "        \"X_train\": X_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_test\": y_test,\n",
    "        \"feature_names\": feature_names,\n",
    "    }\n",
    "\n",
    "    # Save dataset as a dictionary for better organization\n",
    "    with open(\"data/processed/dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(artifacts, f)\n",
    "\n",
    "    # Save scaler separately\n",
    "    with open(\"models/scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8ba189",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load and clean\n",
    "    df = load_features(\"data/raw/features.csv\")\n",
    "\n",
    "    # Feature engineering\n",
    "    df = engineer_features(df)\n",
    "\n",
    "    # Process data\n",
    "    X_train_final, X_test_final, y_train_res, y_test, scaler, feature_names = (\n",
    "        preprocess_pipeline(df)\n",
    "    )\n",
    "\n",
    "    # Inspect data before saving\n",
    "    inspect_data(X_train_final, X_test_final, y_train_res, y_test, feature_names)\n",
    "\n",
    "    # Save artifacts\n",
    "    save_artifacts(\n",
    "        X_train_final, X_test_final, y_train_res, y_test, scaler, feature_names\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
